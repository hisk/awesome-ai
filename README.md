# Awesome AI Resources

A curated list of AI tools, courses, books, and resources for anyone interested in exploring artificial intelligence, machine learning, and deep learning.

# Table of Contents

- [Tools](#tools)
   * [Chat](#chat)
   * [Images](#images)
   * [Video](#video)
   * [Commercial Tools](#commercial-tools)
- [Courses](#courses)
   * [Introductory Courses](#introductory-courses)
   * [Deep Learning and Reinforcement Learning](#deep-learning-and-reinforcement-learning)
   * [Advanced AI and Machine Learning Courses](#advanced-ai-and-machine-learning-courses)
- [Books](#books)
- [Videos](#videos)
- [Learning Resources](#learning-resources)
   * [Online Resources](#online-resources)
   * [Awesome GitHub Resources](#awesome-github-resources)
- [Newsletters](#newsletters)
- [On-Device AI](#on-device-ai)
   * [Evolution of On-Device LLMs](#evolution-of-on-device-llms)
   * [LLM Architecture Foundations](#llm-architecture-foundations)
   * [On-Device LLMs Training](#on-device-llms-training)
   * [Limitations of Cloud-Based LLM Inference and Advantages of On-Device Inference](#limitations-of-cloud-based-llm-inference-and-advantages-of-on-device-inference)
   * [The Performance Indicator of On-Device LLMs](#the-performance-indicator-of-on-device-llms)
   * [Efficient Architectures for On-Device LLMs](#efficient-architectures-for-on-device-llms)
   * [Model Compression and Parameter Sharing](#model-compression-and-parameter-sharing)
   * [Collaborative and Hierarchical Model Approaches](#collaborative-and-hierarchical-model-approaches)
   * [Memory and Computational Efficiency](#memory-and-computational-efficiency)
   * [Mixture-of-Experts (MoE) Architectures](#mixture-of-experts-moe-architectures)
   * [Hybrid Architectures](#hybrid-architectures)
   * [General Efficiency and Performance Improvements](#general-efficiency-and-performance-improvements)



<!-- TOC --><a name="tools"></a>
## Tools

<!-- TOC --><a name="chat"></a>
### Chat
  - [Chat GPT](https://chat.openai.com/) - A free-to-use AI system that allows users to engage in conversations, gain insights, automate tasks, and witness the future of AI all in one place.
  - [Gemini](https://gemini.google.com/) - Direct access to Google AI for writing, planning, learning, and more.
  - [Claude](https://www.anthropic.com/claude) - Foundational AI models for brainstorming ideas, analyzing images, and processing long documents.

<!-- TOC --><a name="images"></a>
### Images
  - [Midjourney](https://www.midjourney.com/) - AI image generation.
  - [DALL·E 3](https://openai.com/dall-e-3) - AI system that creates realistic images and art from a natural-language description.

<!-- TOC --><a name="video"></a>
### Video
  - [Sora](https://openai.com/sora) - Text-to-video AI model that creates imaginative scenes from text.
  - [Runway](https://runwayml.com/) - AI video generation.

<!-- TOC --><a name="commercial-tools"></a>
### Commercial Tools
  - [Taskade](https://www.taskade.com) - Build, train, and deploy AI agents to automate tasks, research, and collaborate in real-time.

<!-- TOC --><a name="courses"></a>
## Courses

<!-- TOC --><a name="introductory-courses"></a>
### Introductory Courses
  - [Introduction to Artificial Intelligence (AI)](https://www.notion.so/owainlewis/Introduction-to-Artificial-Intelligence-AI-ef59b363654542e597ba46a19d129882?pvs=4) - High-level introduction to AI from IBM on Coursera.
  - [Introduction to Generative AI](https://www.coursera.org/learn/introduction-to-generative-ai) - Beginner-level introduction to Generative AI from Google on Coursera.
  - [CS50’s Intro to Artificial Intelligence](https://cs50.harvard.edu/ai/2020) - Concepts and algorithms at the foundation of modern AI.
  - [MIT: Intro to Deep Learning](https://introtodeeplearning.com) - Seven-day bootcamp to introduce deep learning methods.

<!-- TOC --><a name="deep-learning-and-reinforcement-learning"></a>
### Deep Learning and Reinforcement Learning
  - [Deep Blueberry: Deep Learning book](https://mithi.github.io/deep-blueberry) - Basics of deep-learning architectures like CNNs, LSTMs, GANs, and more.
  - [Spinning Up in Deep Reinforcement Learning](https://spinningup.openai.com/) - A free deep reinforcement learning course by OpenAI.
  - [Deep Learning](https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187) - Introductory course to deep learning using TensorFlow.
  - [Stanford Statistical Learning](http://online.stanford.edu/course/statistical-learning-winter-2014) - Introductory course on machine learning.

<!-- TOC --><a name="advanced-ai-and-machine-learning-courses"></a>
### Advanced AI and Machine Learning Courses
  - [Knowledge Based Artificial Intelligence](https://www.udacity.com/course/knowledge-based-ai-cognitive-systems--ud409) - Georgia Tech's course focusing on Symbolic AI.
  - [Machine Learning Crash Course by Google](https://developers.google.com/machine-learning/crash-course/ml-intro) - A series of lessons with video lectures, real-world case studies, and hands-on practice.
  - [Deep RL Bootcamp Lectures](https://sites.google.com/view/deep-rl-bootcamp/lectures) - Deep Reinforcement Bootcamp Lectures - August 2017.
  - [Elements of AI](https://www.elementsofai.com/) - An introduction to AI for everyone interested in learning what AI is and how it affects our lives.

<!-- TOC --><a name="books"></a>
## Books

- [Deep Learning](http://www.deeplearningbook.org/) - Introduction by Goodfellow, Bengio, and Courville covering deep learning techniques and perspectives.
- [The Hundred-Page Machine Learning Book](http://themlbook.com/) - A comprehensive guide to machine learning in just 100 pages.
- [Generative AI in Action](https://www.manning.com/books/generative-ai-in-action) - Learn how to add generative AI tools for text, images, and code into projects.

<!-- TOC --><a name="videos"></a>
## Videos

- [The Unreasonable Effectiveness Of Deep Learning](https://www.youtube.com/watch?v=sc-KbuZqGkI) - Yann LeCun gives a talk on deep convolutional neural networks.
- [AWS Machine Learning in Motion](https://www.manning.com/livevideo/aws-machine-learning-in-motion) - Learn how to build a predictive algorithm using AWS.
- [Reinforcement Learning in Motion](https://www.manning.com/livevideo/reinforcement-learning-in-motion) - Concepts like how RL systems learn and how to train AI agents.

<!-- TOC --><a name="learning-resources"></a>
## Learning Resources

<!-- TOC --><a name="online-resources"></a>
### Online Resources
  - [Neural Networks And Deep Learning](http://neuralnetworksanddeeplearning.com) - Learn the core concepts behind neural networks and deep learning.
  - [Awesome Machine Learning](https://github.com/josephmisiti/awesome-machine-learning) - Curated list of machine learning resources.
  - [Awesome Deep Learning Resources](https://github.com/guillaume-chevalier/awesome-deep-learning-resources) - Rough list of deep learning resources.
  - [Professional and In-Depth AI Video Courses](https://freecoursesite.com/?s=Artificial+Intelligence) - Free professional AI tutorials and courses.
  - [MIT: TinyML and Efficient Deep Learning Computing](https://efficientml.ai)
  - [Harvard: Machine Learning Systems](https://mlsysbook.ai/)
  - [Deep Learning AI: Introduction to On-Device AI](https://www.deeplearning.ai/short-courses/introduction-to-on-device-ai/)

<!-- TOC --><a name="awesome-github-resources"></a>
### Awesome GitHub Resources
  - [Awesome Graph Classification](https://github.com/benedekrozemberczki/awesome-graph-classification) - Learning from graph-structured data.
  - [Awesome Fraud Detection Papers](https://github.com/benedekrozemberczki/awesome-fraud-detection-papers) - Fraud detection papers from machine learning conferences.

<!-- TOC --><a name="newsletters"></a>
## Newsletters

- [Superhuman.ai](https://www.superhuman.ai/) - A daily AI newsletter.

<!-- TOC --><a name="on-device-ai"></a>
## On-Device AI

<!-- TOC --><a name="evolution-of-on-device-llms"></a>
### Evolution of On-Device LLMs
  - [Tinyllama](https://arxiv.org/abs/2401.02385) - Open-source small language model.
  - [MobileVLM V2](https://arxiv.org/abs/2402.03766) - Faster and stronger baseline for Vision Language Model.
  - [MobileAIBench](https://arxiv.org/abs/2406.10290) - Benchmarking LLMs and LMMs for on-device use cases.
  - [Octopus series papers](https://arxiv.org/abs/2404.01549) - On-device language models for different applications. [[Octopus v2]](https://arxiv.org/abs/2404.01744) [[Octopus v3]](https://arxiv.org/abs/2404.11459) [[Octopus v4]](https://arxiv.org/abs/2404.19296) [[Github]](https://github.com/NexaAI).
  - [The Era of 1-bit LLMs](https://arxiv.org/abs/2402.17764) - All large language models are in 1.58 bits.
  - [AWQ](https://arxiv.org/abs/2306.00978) - Activation-aware weight quantization for LLM compression and acceleration. [[Github]](https://github.com/mit-han-lab/llm-awq).
  - [Small Language Models](https://arxiv.org/pdf/2409.15790) - Survey, measurements, and insights.

<!-- TOC --><a name="llm-architecture-foundations"></a>
### LLM Architecture Foundations
  - [The case for 4-bit precision](https://arxiv.org/abs/2212.09720) - k-bit inference scaling laws.
  - [Challenges and applications of large language models](https://arxiv.org/abs/2307.10169).
  - [MiniLLM](https://arxiv.org/abs/2306.08543) - Knowledge distillation of large language models. [[Github]](https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs).
  - [Gptq](https://arxiv.org/abs/2210.17323) - Accurate post-training quantization for generative pre-trained transformers. [[Github]](https://github.com/IST-DASLab/gptq).
  - [Gpt3.int8()](https://arxiv.org/abs/2208.07339) - 8-bit matrix multiplication for transformers at scale.

<!-- TOC --><a name="on-device-llms-training"></a>
### On-Device LLMs Training
  - [OpenELM](https://arxiv.org/abs/2404.14619) - An efficient language model family with open training and inference framework. [[Github]](https://github.com/apple/corenet).

<!-- TOC --><a name="limitations-of-cloud-based-llm-inference-and-advantages-of-on-device-inference"></a>
### Limitations of Cloud-Based LLM Inference and Advantages of On-Device Inference
  - [Ferret-v2](https://arxiv.org/abs/2404.07973) - An improved baseline for referring and grounding with large language models.
  - [Phi-3 Technical Report](https://arxiv.org/abs/2404.14219) - A highly capable language model locally on your phone.
  - [Exploring post-training quantization](https://arxiv.org/abs/2303.08302) - Comprehensive study to low rank compensation.
  - [Matrix compression](https://arxiv.org/abs/2310.11028) - Randomized low rank and low precision factorization. [[Github]](https://github.com/pilancilab/matrix-compressor).

<!-- TOC --><a name="the-performance-indicator-of-on-device-llms"></a>
### The Performance Indicator of On-Device LLMs
  - [MNN](https://github.com/alibaba/MNN) - A lightweight deep neural network inference engine.
  - [PowerInfer-2](https://arxiv.org/abs/2406.06282) - Fast large language model inference on a smartphone. [[Github]](https://github.com/SJTU-IPADS/PowerInfer).
  - [llama.cpp](https://github.com/ggerganov/llama.cpp) - Lightweight library for approximate nearest neighbors and maximum inner product search.
  - [Powerinfer](https://arxiv.org/abs/2312.12456) - Fast large language model serving with a consumer-grade GPU. [[Github]](https://github.com/SJTU-IPADS/PowerInfer).

<!-- TOC --><a name="efficient-architectures-for-on-device-llms"></a>
### Efficient Architectures for On-Device LLMs
  - [MobileLLM](https://arxiv.org/abs/2402.14905) - High accuracy, optimized for sub-billion parameter models, embedding sharing, grouped-query attention, reduced model size.
  - [EdgeShard](https://arxiv.org/abs/2405.14371) - Up to 50% latency reduction, collaborative edge-cloud computing, optimal shard placement, distributed model components reduce individual device load.
  - [LLMCad](https://arxiv.org/abs/2309.04255) - Up to 9.3× speedup in token generation, generate-then-verify, token tree generation, smaller LLM for token generation, larger LLM for verification.
  - [Any-Precision LLM](https://arxiv.org/abs/2402.10517) - Supports multiple precisions efficiently, post-training quantization, memory-efficient design, substantial memory savings with versatile model precisions.
  - [Breakthrough Memory](https://ieeexplore.ieee.org/abstract/document/10477465) - Up to 4.5× performance improvement, PIM and PNM technologies enhance memory processing, enhanced memory bandwidth and capacity.
  - [MELTing Point](https://arxiv.org/abs/2403.12844) - Provides systematic performance evaluation, analyzes impacts of quantization, efficient model evaluation, evaluates memory and computational efficiency trade-offs.
  - [LLMaaS on device](https://arxiv.org/abs/2403.11805) - Reduces context switching latency significantly, stateful execution, fine-grained KV cache compression, efficient memory management with tolerance-aware compression and swapping.
  - [LocMoE](https://arxiv.org/abs/2401.13920) - Reduces training time per epoch by up to 22.24%, orthogonal gating weights, locality-based expert regularization, minimizes communication overhead with group-wise All-to-All and recompute pipeline.
  - [EdgeMoE](https://arxiv.org/abs/2308.14352) - Significant performance improvements on edge devices, expert-wise bitwidth adaptation, preloading experts, efficient memory management through expert-by-expert computation reordering.
  - [JetMoE](https://arxiv.org/abs/2404.07413) - Outperforms Llama27B and 13B-Chat with fewer parameters, reduces inference computation by 70% using sparse activation, 8B total parameters, only 2B activated per input token.
  - [Pangu-$\pi$ Pro](https://arxiv.org/abs/2402.02791) - Neural architecture, parameter initialization, and optimization strategy for billion-level parameter models, embedding sharing, tokenizer compression, reduced model size via architecture tweaking.
  - [Zamba2](https://www.zyphra.com/post/zamba2-small) - 2x faster time-to-first-token, a 27% reduction in memory overhead, and a 1.29x lower generation latency compared to Phi3-3.8B, hybrid Mamba2/Attention architecture and shared transformer block, 2.7B parameters, fewer KV-states due to reduced attention.

<!-- TOC --><a name="model-compression-and-parameter-sharing"></a>
### Model Compression and Parameter Sharing
  - [AWQ](https://arxiv.org/abs/2306.00978) - Activation-aware weight quantization for LLM compression and acceleration. [[Github]](https://github.com/mit-han-lab/llm-awq).
  - [MobileLLM](https://arxiv.org/abs/2402.14905) - Optimizing sub-billion parameter language models for on-device use cases. [[Github]](https://github.com/facebookresearch/MobileLLM).

<!-- TOC --><a name="collaborative-and-hierarchical-model-approaches"></a>
### Collaborative and Hierarchical Model Approaches
  - [EdgeShard](https://arxiv.org/abs/2405.14371) - Efficient LLM inference via collaborative edge computing.
  - [Llmcad](https://arxiv.org/abs/2309.04255) - Fast and scalable on-device large language model inference.

<!-- TOC --><a name="memory-and-computational-efficiency"></a>
### Memory and Computational Efficiency
  - [Breakthrough Memory Solutions](https://ieeexplore.ieee.org/document/10477465) - Improved performance on LLM inference.
  - [MELTing Point](https://arxiv.org/abs/2403.12844) - Mobile evaluation of language transformers. [[Github]](https://github.com/brave-experiments/MELT-public).

<!-- TOC --><a name="mixture-of-experts-moe-architectures"></a>
### Mixture-of-Experts (MoE) Architectures
  - [LLM as a System Service](https://arxiv.org/abs/2403.11805) - On mobile devices.
  - [Locmoe](https://arxiv.org/abs/2401.13920) - Low-overhead MoE for large language model training.
  - [Edgemoe](https://arxiv.org/abs/2308.14352) - Fast on-device inference of MoE-based large language models.

<!-- TOC --><a name="hybrid-architectures"></a>
### Hybrid Architectures
  - [Zamba2](https://www.zyphra.com/post/zamba2-small) - Hybrid Mamba2 and attention models for on-device. [[Zamba2-2.7B]](https://www.zyphra.com/post/zamba2-small) [[Zamba2-1.2B]](https://www.zyphra.com/post/zamba2-mini).

<!-- TOC --><a name="general-efficiency-and-performance-improvements"></a>
### General Efficiency and Performance Improvements
  - [Any-Precision LLM](https://www.arxiv.org/pdf/2402.10517) - Low-cost deployment of multiple, different-sized LLMs. [[Github]](https://github.com/SNU-ARC/any-precision-llm).
  - [On the Viability of Using LLMs for SW/HW Co-design](https://arxiv.org/abs/2306.06923) - An example in designing CIM DNN accelerators.

